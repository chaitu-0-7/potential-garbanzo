name: Job Scraper Scheduler

on:
  schedule:
    - cron: '30 1 * * *'
    - cron: '30 2 * * *'
    - cron: '30 3 * * *'
    - cron: '30 4 * * *'
    - cron: '30 5 * * *'
    - cron: '30 6 * * *'
    - cron: '30 7 * * *'
    - cron: '30 8 * * *'
    - cron: '30 9 * * *'
    - cron: '30 10 * * *'
    - cron: '30 11 * * *'
    - cron: '30 12 * * *'
    - cron: '30 13 * * *'
    - cron: '30 14 * * *'
    - cron: '30 15 * * *'
    - cron: '30 16 * * *'
    - cron: '30 17 * * *'
  workflow_dispatch:

jobs:
  scrape-and-match:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python 3.10
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Install Playwright browsers
        run: |
          playwright install chromium
          playwright install-deps chromium
      
      - name: Create directories
        run: |
          mkdir -p scraped_data
          mkdir -p logs
      
      - name: Run job scraper
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
          DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
          MONGO_URI: ${{ secrets.MONGO_URI }}
          LINKEDIN_URL: ${{ secrets.LINKEDIN_URL }}
          RESUME_PATH: ${{ secrets.RESUME_PATH }}
          SCHEDULER_ENABLED: "false"
          SCHEDULER_TIMEZONE: "Asia/Kolkata"
          SCRAPE_MAX_JOBS: "30"
          MIN_MATCH_SCORE: "60"
        run: |
          python -c "from new import scrape_and_match_task; scrape_and_match_task(is_hourly_run=True)"
      
      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs-${{ github.run_number }}
          path: |
            logs/
            scraped_data/*.json
          retention-days: 7
      
      - name: Notify on failure
        if: failure()
        run: |
          curl -H "Content-Type: application/json" -X POST \
            -d "{\"content\":\"‚ùå Job scraper failed! Run: ${{ github.run_id }}\"}" \
            "${{ secrets.DISCORD_WEBHOOK_URL }}"
