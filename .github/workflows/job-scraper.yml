name: Job Scraper Scheduler

on:
  schedule:
    # Run every 30 minutes from 08:00 IST through 23:00 IST (inclusive).
    # IST = UTC+5:30. To cover both :00 and :30 IST times we add two cron lines in UTC.
    # - At IST xx:00 -> UTC (xx-5):30 -> minute 30 of UTC hour range 2-17 (for IST 08-23)
    # - At IST xx:30 -> UTC (xx-5):00 -> minute 0  of UTC hour range 3-18 (for IST 08-23)
    - cron: '30 2-17 * * *'
    - cron: '0 3-18 * * *'
  workflow_dispatch:

jobs:
  scrape-and-match:
    runs-on: ubuntu-22.04
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python 3.10
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Install Playwright browsers
        run: |
          playwright install chromium
          playwright install-deps chromium
      
      - name: Create directories
        run: |
          mkdir -p scraped_data
          mkdir -p logs
      
      - name: Run job scraper
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
          DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
          MONGO_URI: ${{ secrets.MONGO_URI }}
          LINKEDIN_URL: ${{ secrets.LINKEDIN_URL }}
          RESUME_PATH: ${{ secrets.RESUME_PATH }}
          SCHEDULER_ENABLED: "false"
          SCHEDULER_TIMEZONE: "Asia/Kolkata"
          SCRAPE_MAX_JOBS: "30"
          MIN_MATCH_SCORE: "60"
        run: |
          python -c "from new import scrape_and_match_task; scrape_and_match_task(is_hourly_run=True)"
      
      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs-${{ github.run_number }}
          path: |
            logs/
            scraped_data/*.json
          retention-days: 7
      
      - name: Notify on failure
        if: failure()
        run: |
          curl -H "Content-Type: application/json" -X POST \
            -d "{\"content\":\"‚ùå Job scraper failed! Run: ${{ github.run_id }}\"}" \
            "${{ secrets.DISCORD_WEBHOOK_URL }}"
